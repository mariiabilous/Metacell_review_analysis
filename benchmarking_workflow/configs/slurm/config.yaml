cluster-generic-submit-cmd:
  mkdir -p logs/{rule} &&
  sbatch
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --time={resources.time}
    --job-name=smk-{rule}-{wildcards}
    --output=logs/{rule}/{rule}-{wildcards}-%j.out
    --error=logs/{rule}/{rule}-{wildcards}-%j.err
restart-times: 1 # this will allow to re-run a job 3 time maximum if it fails (usefull if we specify in the rule to increase memory in each attempt)
latency-wait: 120
jobs: 500 # to say that we wwant that a maximum of 500 jobs are launched on the cluster
keep-going: True  #Go on with independent jobs if a job fails
rerun-incomplete: True  #Re-run all jobs the output of which is recognized as incomplete
printshellcmds: True  #Print out the shell commands that will be executed
use-apptainer: True  #If defined in the rule, run job within a singularity container. If this flag is not set, the singularity directive is ignored
executor: cluster-generic

default-resources:
  mem_mb: 8000
  time: 60
set-threads:
  dataset_download: 1
  dataset_aggregating: 1
  run_SuperCell: 16
  run_SEACells: 16
  run_MetaCell: 16
set-resources:
  dataset_aggregating:
    mem_mb: 50000
  run_SuperCell:
    mem_mb: attempt * 150000
    time: 180
  run_SEACells:
    mem_mb: attempt * 150000
    time: 180
  run_MetaCell:
    mem_mb: attempt * 150000
    time: 180
