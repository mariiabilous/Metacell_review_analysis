cluster-generic-submit-cmd:
  mkdir -p logs/{rule} &&
  sbatch
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --time={resources.time}
    --job-name=smk-{rule}-{wildcards}
    --partition={resources.partition}
    --gpus={resources.gpus}
    --output=logs/{rule}/{rule}-{wildcards}-%j.out
    --error=logs/{rule}/{rule}-{wildcards}-%j.err
restart-times: 0 # this will allow to re-run a job 3 time maximum if it fails (usefull if we specify in the rule to increase memory in each attempt)
latency-wait: 120
jobs: 500 # to say that we wwant that a maximum of 500 jobs are launched on the cluster
keep-going: True  #Go on with independent jobs if a job fails
rerun-incomplete: True  #Re-run all jobs the output of which is recognized as incomplete
printshellcmds: True  #Print out the shell commands that will be executed
use-apptainer: True  #If defined in the rule, run job within a singularity container. If this flag is not set, the singularity directive is ignored
executor: cluster-generic
apptainer-args: '--bind /work --nv'

default-resources:
  mem_mb: 8000
  time: 60
  partition: 'cpu'
  gpus: 0
set-threads:
  dataset_download: 1
  dataset_subsetting: 1
  run_downstream_analysis: 1
set-resources:
  dataset_subsetting:
    mem_mb: attempt * 100000
  dataset_splitting:
    mem_mb: attempt * 100000
